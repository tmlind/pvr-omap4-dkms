From tony Mon Sep 17 00:00:00 2001
From: Rob Clark <rob@ti.com>
Date: Mon, 3 Feb 2020 10:38:55 -0800
Subject: [PATCH] HACK: drm/omap: Add omapdrm plugin API

This patch enables SGX driver to be added as a plugin to omapdrm. This
is a HACK, and must be reverted/rewritten when proper DRM-SGX interop is
implemented.

As this is a HACK, the code is wrapped with CONFIG_DRM_OMAP_SGX_PLUGIN,
and grouped together and separated from the main omapdrm code when
possible. This means checkpatch will complain a bit, but this should
make it easier to revert this later and to avoid conflicts with
mainline.

Main changes involved:

1. SGX specific GEM VM operations
	SGX requires contiguous memory for both texture memory as well
	as framebuffers.

	Memory allocation of FB is done through omapdrm and is guaranteed
	to be contiguous.

	Texture memory can come from:
	a. user space allocated memory
	b. memory is allocated by other cores
	c. memory comes from CMA

	Texture memory can be non-contiguous, but is wrapped as GEM
	objects which enables the use of TILER to map them as contiguous
	memory to the SGX HW.

2. Support for ioctls from plugin driver
	SGX driver registers as a plugin to the omapdrm driver. During
	registration, SGX specific ioctls are added to omapdrm. This allows
	user space to control specific SGX feature sets using the DRM FD.

3. Export GEM functions
	SGX driver needs to work directly on GEM objects for DSS
	synchronization, getting TILER address, etc.

Signed-off-by: Rob Clark <rob@ti.com>
Signed-off-by: Subhajit Paul <subhajit_paul@ti.com>
Signed-off-by: Tomi Valkeinen <tomi.valkeinen@ti.com>
Signed-off-by: Anand Balagopalakrishnan <anandb@ti.com>
Signed-off-by: Jyri Sarha <jsarha@ti.com>
[tony@atomide.com:
 - Reverted Laurent and Tomi's patches:
   aa0408bcb1b8 ("drm: omapdrm: Remove remap argument to omap_gem_get_paddr()")
   d6f544f6bf41 ("drm: omapdrm: Remove legacy buffer synchronization support")
   3f50effdb835 ("drm/omap: remove support for ext mem & sync")
 - Updated patch against v5.4-rc series
 - Pass file in omap_gem_new_ext() to deal with drm_vma_node_allow()]
 - Add CONFIG_PREEMPT=y to omap2plus_defconfig
 - Add SGX dts child node into omap4.dtsi
 - Update dsi command mode panels from omap_gem_op_finish()
 - Add CONFIG_DRM_LEGACY=y for CONFIG_DRM_VM=y
 - Ignore ERESTARTSYS to avoid unpaired ioctl_gem_cpu_prep ioctl_gem_cpu_fini
 - Make omap_gem_set_sync_object a nop for PVRSRV_DISABLE_UM_SYNCOBJ_MAPPINGS
 - Remove waiter->read/write_target]
 - Fix alignment on the tiler sgx path
 - Update to use OMAP_BO_TILED_MASK
 - Add back reverted omapdrm commit:
   23b482252836 ("drm/omap: add OMAP_BO flags to affect buffer allocation")
]
Not-yet-signed-off-by: Tony Lindgren <tony@atomide.com>
---
 arch/arm/boot/dts/omap4.dtsi              |   9 +-
 arch/arm/configs/omap2plus_defconfig      |   2 +
 drivers/gpu/drm/omapdrm/omap_drv.c        | 198 +++++++++-
 drivers/gpu/drm/omapdrm/omap_fb.c         |   2 +-
 drivers/gpu/drm/omapdrm/omap_fbdev.c      |   2 +-
 drivers/gpu/drm/omapdrm/omap_gem.c        | 437 +++++++++++++++++++++-
 drivers/gpu/drm/omapdrm/omap_gem.h        |   8 -
 drivers/gpu/drm/omapdrm/omap_gem_dmabuf.c |   2 +-
 include/uapi/drm/omap_drm.h               |  88 ++++-
 9 files changed, 715 insertions(+), 33 deletions(-)

diff --git a/arch/arm/boot/dts/omap4.dtsi b/arch/arm/boot/dts/omap4.dtsi
--- a/arch/arm/boot/dts/omap4.dtsi
+++ b/arch/arm/boot/dts/omap4.dtsi
@@ -347,10 +347,11 @@ target-module@56000000 {
 			#size-cells = <1>;
 			ranges = <0 0x56000000 0x2000000>;
 
-			/*
-			 * Closed source PowerVR driver, no child device
-			 * binding or driver in mainline
-			 */
+			sgx: img@0 {
+				compatible = "ti,omap4-sgx540-120", "img,sgx540-120", "img,sgx540";
+				reg = <0x0 0x2000000>;	/* 32MB */
+				interrupts = <GIC_SPI 21 IRQ_TYPE_LEVEL_HIGH>;
+			};
 		};
 
 		dss: dss@58000000 {
diff --git a/arch/arm/configs/omap2plus_defconfig b/arch/arm/configs/omap2plus_defconfig
--- a/arch/arm/configs/omap2plus_defconfig
+++ b/arch/arm/configs/omap2plus_defconfig
@@ -4,6 +4,7 @@ CONFIG_POSIX_MQUEUE=y
 CONFIG_AUDIT=y
 CONFIG_NO_HZ=y
 CONFIG_HIGH_RES_TIMERS=y
+CONFIG_PREEMPT=y
 CONFIG_BSD_PROCESS_ACCT=y
 CONFIG_IKCONFIG=y
 CONFIG_IKCONFIG_PROC=y
@@ -370,6 +371,7 @@ CONFIG_DRM_PANEL_SHARP_LS037V7DW01=m
 CONFIG_DRM_PANEL_SONY_ACX565AKM=m
 CONFIG_DRM_PANEL_TPO_TD028TTEC1=m
 CONFIG_DRM_PANEL_TPO_TD043MTEA1=m
+CONFIG_DRM_LEGACY=y
 CONFIG_FB=y
 CONFIG_FIRMWARE_EDID=y
 CONFIG_FB_MODE_HELPERS=y
diff --git a/drivers/gpu/drm/omapdrm/omap_drv.c b/drivers/gpu/drm/omapdrm/omap_drv.c
--- a/drivers/gpu/drm/omapdrm/omap_drv.c
+++ b/drivers/gpu/drm/omapdrm/omap_drv.c
@@ -31,6 +31,15 @@
 #define DRIVER_MINOR		0
 #define DRIVER_PATCHLEVEL	0
 
+static struct drm_device *drm_device;
+
+static struct omap_drm_plugin *sgx_plugin;
+
+/* keep track of whether we are already loaded.. we may need to call
+ * plugin's load() if they register after we are already loaded
+ */
+static bool drm_loaded;
+
 /*
  * mode config funcs
  */
@@ -437,6 +446,19 @@ static int ioctl_get_param(struct drm_device *dev, void *data,
 
 #define OMAP_BO_USER_MASK	0x00ffffff	/* flags settable by userspace */
 
+static int ioctl_get_base(struct drm_device *dev, void *data,
+		struct drm_file *file_priv)
+{
+	struct drm_omap_get_base *args = data;
+
+	if (!sgx_plugin)
+		return -ENODEV;
+
+	args->ioctl_base = sgx_plugin->ioctl_base;
+
+	return 0;
+}
+
 static int ioctl_gem_new(struct drm_device *dev, void *data,
 		struct drm_file *file_priv)
 {
@@ -450,6 +472,57 @@ static int ioctl_gem_new(struct drm_device *dev, void *data,
 				   &args->handle);
 }
 
+static int ioctl_gem_cpu_prep(struct drm_device *dev, void *data,
+		struct drm_file *file_priv)
+{
+	struct drm_omap_gem_cpu_prep *args = data;
+	struct drm_gem_object *obj;
+	int ret;
+
+	VERB("%p:%p: handle=%d, op=%x", dev, file_priv, args->handle, args->op);
+
+	obj = drm_gem_object_lookup(file_priv, args->handle);
+	if (!obj)
+		return -ENOENT;
+
+	ret = omap_gem_op_sync(obj, args->op);
+	if (ret && ret != -ERESTARTSYS)
+		goto put_unlocked;
+
+	/*
+	 * Just ignore -ERESTARTSYS from omap_gem_op_sync() and continue in
+	 * order to keep ioctl_gem_cpu_prep() and ioctl_gem_cpu_fini() calls
+	 * paired for usage counts. Otherwise rotating a full screen OpenGL ES
+	 * app with xrandr a few times fails.
+	 */
+	ret = omap_gem_op_start(obj, args->op);
+
+put_unlocked:
+	drm_gem_object_put_unlocked(obj);
+
+	return ret;
+}
+
+static int ioctl_gem_cpu_fini(struct drm_device *dev, void *data,
+		struct drm_file *file_priv)
+{
+	struct drm_omap_gem_cpu_fini *args = data;
+	struct drm_gem_object *obj;
+	int ret;
+
+	VERB("%p:%p: handle=%d", dev, file_priv, args->handle);
+
+	obj = drm_gem_object_lookup(file_priv, args->handle);
+	if (!obj)
+		return -ENOENT;
+
+	ret = omap_gem_op_finish(obj, args->op);
+
+	drm_gem_object_put_unlocked(obj);
+
+	return ret;
+}
+
 static int ioctl_gem_info(struct drm_device *dev, void *data,
 		struct drm_file *file_priv)
 {
@@ -471,19 +544,19 @@ static int ioctl_gem_info(struct drm_device *dev, void *data,
 	return ret;
 }
 
-static const struct drm_ioctl_desc ioctls[DRM_COMMAND_END - DRM_COMMAND_BASE] = {
+static struct drm_ioctl_desc ioctls[DRM_COMMAND_END - DRM_COMMAND_BASE] = {
 	DRM_IOCTL_DEF_DRV(OMAP_GET_PARAM, ioctl_get_param,
 			  DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(OMAP_SET_PARAM, drm_invalid_op,
 			  DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF_DRV(OMAP_GET_BASE, ioctl_get_base,
+			  DRM_UNLOCKED|DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(OMAP_GEM_NEW, ioctl_gem_new,
 			  DRM_RENDER_ALLOW),
-	/* Deprecated, to be removed. */
-	DRM_IOCTL_DEF_DRV(OMAP_GEM_CPU_PREP, drm_noop,
-			  DRM_RENDER_ALLOW),
-	/* Deprecated, to be removed. */
-	DRM_IOCTL_DEF_DRV(OMAP_GEM_CPU_FINI, drm_noop,
-			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(OMAP_GEM_CPU_PREP, ioctl_gem_cpu_prep,
+			  DRM_AUTH | DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(OMAP_GEM_CPU_FINI, ioctl_gem_cpu_fini,
+			  DRM_AUTH | DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(OMAP_GEM_INFO, ioctl_gem_info,
 			  DRM_RENDER_ALLOW),
 };
@@ -492,19 +565,50 @@ static const struct drm_ioctl_desc ioctls[DRM_COMMAND_END - DRM_COMMAND_BASE] =
  * drm driver funcs
  */
 
+static int dev_load(struct drm_device *dev, unsigned long flags)
+{
+	drm_device = dev;
+
+	drm_loaded = true;
+
+	if (sgx_plugin && sgx_plugin->load)
+		sgx_plugin->load(dev, flags);
+
+	return 0;
+}
+
+static void dev_unload(struct drm_device *dev)
+{
+	if (sgx_plugin && sgx_plugin->unload)
+		sgx_plugin->unload(dev);
+
+	drm_loaded = false;
+}
+
 static int dev_open(struct drm_device *dev, struct drm_file *file)
 {
 	file->driver_priv = NULL;
 
 	DBG("open: dev=%p, file=%p", dev, file);
 
+	if (sgx_plugin && sgx_plugin->open)
+		sgx_plugin->open(dev, file);
+
 	return 0;
 }
 
+static void dev_preclose(struct drm_device *dev, struct drm_file *file)
+{
+	if (sgx_plugin && sgx_plugin->release)
+		sgx_plugin->release(dev, file);
+
+	kfree(file->driver_priv);
+}
+
 static const struct vm_operations_struct omap_gem_vm_ops = {
 	.fault = omap_gem_fault,
-	.open = drm_gem_vm_open,
-	.close = drm_gem_vm_close,
+	.open = omap_gem_vm_open,
+	.close = omap_gem_vm_close,
 };
 
 static const struct file_operations omapdriver_fops = {
@@ -522,7 +626,10 @@ static const struct file_operations omapdriver_fops = {
 static struct drm_driver omap_drm_driver = {
 	.driver_features = DRIVER_MODESET | DRIVER_GEM  |
 		DRIVER_ATOMIC | DRIVER_RENDER,
+	.load = dev_load,
+	.unload = dev_unload,
 	.open = dev_open,
+	.preclose = dev_preclose,
 	.lastclose = drm_fb_helper_lastclose,
 #ifdef CONFIG_DEBUG_FS
 	.debugfs_init = omap_debugfs_init,
@@ -554,6 +661,79 @@ static const struct soc_device_attribute omapdrm_soc_devices[] = {
 	{ /* sentinel */ }
 };
 
+int omap_drm_register_plugin(struct omap_drm_plugin *plugin)
+{
+	struct drm_device *dev = drm_device;
+	int i;
+
+	DBG("register plugin: %p (%s)", plugin, plugin->name);
+
+	if (sgx_plugin)
+		return -EBUSY;
+
+	for (i = 0; i < plugin->num_ioctls; i++) {
+		int nr = i + DRM_OMAP_NUM_IOCTLS;
+
+		/* check for out of bounds ioctl or already registered ioctl */
+		if (nr > ARRAY_SIZE(ioctls) || ioctls[nr].func) {
+			dev_err(dev->dev, "invalid ioctl: %d (nr=%d)\n", i, nr);
+			return -EINVAL;
+		}
+	}
+
+	plugin->ioctl_base = DRM_OMAP_NUM_IOCTLS;
+
+	/* register the plugin's ioctl's */
+	for (i = 0; i < plugin->num_ioctls; i++) {
+		int nr = i + DRM_OMAP_NUM_IOCTLS;
+
+		DBG("register ioctl: %d %08x", nr, plugin->ioctls[i].cmd);
+
+		ioctls[nr] = plugin->ioctls[i];
+	}
+
+	omap_drm_driver.num_ioctls = DRM_OMAP_NUM_IOCTLS + plugin->num_ioctls;
+
+	sgx_plugin = plugin;
+
+	if (drm_loaded && plugin->load)
+		plugin->load(dev, 0);
+
+	return 0;
+}
+EXPORT_SYMBOL(omap_drm_register_plugin);
+
+int omap_drm_unregister_plugin(struct omap_drm_plugin *plugin)
+{
+	int i;
+
+	for (i = 0; i < plugin->num_ioctls; i++) {
+		const struct drm_ioctl_desc empty = { 0 };
+		int nr = i + DRM_OMAP_NUM_IOCTLS;
+
+		ioctls[nr] = empty;
+	}
+
+	omap_drm_driver.num_ioctls = DRM_OMAP_NUM_IOCTLS;
+
+	sgx_plugin = NULL;
+
+	return 0;
+}
+EXPORT_SYMBOL(omap_drm_unregister_plugin);
+
+void *omap_drm_file_priv(struct drm_file *file)
+{
+	return file->driver_priv;
+}
+EXPORT_SYMBOL(omap_drm_file_priv);
+
+void omap_drm_file_set_priv(struct drm_file *file, void *priv)
+{
+	file->driver_priv = priv;
+}
+EXPORT_SYMBOL(omap_drm_file_set_priv);
+
 static int omapdrm_init(struct omap_drm_private *priv, struct device *dev)
 {
 	const struct soc_device_attribute *soc;
diff --git a/drivers/gpu/drm/omapdrm/omap_fb.c b/drivers/gpu/drm/omapdrm/omap_fb.c
--- a/drivers/gpu/drm/omapdrm/omap_fb.c
+++ b/drivers/gpu/drm/omapdrm/omap_fb.c
@@ -236,7 +236,7 @@ int omap_framebuffer_pin(struct drm_framebuffer *fb)
 
 	for (i = 0; i < n; i++) {
 		struct plane *plane = &omap_fb->planes[i];
-		ret = omap_gem_pin(fb->obj[i], &plane->dma_addr);
+		ret = omap_gem_pin(fb->obj[i], &plane->dma_addr, true);
 		if (ret)
 			goto fail;
 		omap_gem_dma_sync_buffer(fb->obj[i], DMA_TO_DEVICE);
diff --git a/drivers/gpu/drm/omapdrm/omap_fbdev.c b/drivers/gpu/drm/omapdrm/omap_fbdev.c
--- a/drivers/gpu/drm/omapdrm/omap_fbdev.c
+++ b/drivers/gpu/drm/omapdrm/omap_fbdev.c
@@ -153,7 +153,7 @@ static int omap_fbdev_create(struct drm_fb_helper *helper,
 	 * to it).  Then we just need to be sure that we are able to re-
 	 * pin it in case of an opps.
 	 */
-	ret = omap_gem_pin(fbdev->bo, &dma_addr);
+	ret = omap_gem_pin(fbdev->bo, &dma_addr, true);
 	if (ret) {
 		dev_err(dev->dev, "could not pin framebuffer\n");
 		ret = -ENOMEM;
diff --git a/drivers/gpu/drm/omapdrm/omap_gem.c b/drivers/gpu/drm/omapdrm/omap_gem.c
--- a/drivers/gpu/drm/omapdrm/omap_gem.c
+++ b/drivers/gpu/drm/omapdrm/omap_gem.c
@@ -23,7 +23,9 @@
 /* note: we use upper 8 bits of flags for driver-internal flags: */
 #define OMAP_BO_MEM_DMA_API	0x01000000	/* memory allocated with the dma_alloc_* API */
 #define OMAP_BO_MEM_SHMEM	0x02000000	/* memory allocated through shmem backing */
+#define OMAP_BO_MEM_EXT		0x04000000	/* memory allocated externally */
 #define OMAP_BO_MEM_DMABUF	0x08000000	/* memory imported from a dmabuf */
+#define OMAP_BO_EXT_SYNC	0x10000000	/* externally allocated sync object */
 
 struct omap_gem_object {
 	struct drm_gem_object base;
@@ -93,6 +95,37 @@ struct omap_gem_object {
 	 * Virtual address, if mapped.
 	 */
 	void *vaddr;
+
+	struct omap_gem_vm_ops *ops;
+
+	struct drm_file *file;
+	u32 *handle;
+
+	/* per-mapper private data. */
+	void *priv;
+
+	/**
+	 * sync-object allocated on demand (if needed)
+	 *
+	 * Per-buffer sync-object for tracking pending and completed hw/dma
+	 * read and write operations.  The layout in memory is dictated by
+	 * the SGX firmware, which uses this information to stall the command
+	 * stream if a surface is not ready yet.
+	 *
+	 * Note that when buffer is used by SGX, the sync-object needs to be
+	 * allocated from a special heap of sync-objects.  This way many sync
+	 * objects can be packed in a page, and not waste GPU virtual address
+	 * space.  Because of this we have to have a omap_gem_set_sync_object()
+	 * API to allow replacement of the syncobj after it has (potentially)
+	 * already been allocated.  A bit ugly but I haven't thought of a
+	 * better alternative.
+	 */
+	struct {
+		uint32_t write_pending;
+		uint32_t write_complete;
+		uint32_t read_pending;
+		uint32_t read_complete;
+	} *sync;
 };
 
 #define to_omap_bo(x) container_of(x, struct omap_gem_object, base)
@@ -133,6 +166,7 @@ struct omap_drm_usergart {
 /** get mmap offset */
 u64 omap_gem_mmap_offset(struct drm_gem_object *obj)
 {
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
 	struct drm_device *dev = obj->dev;
 	int ret;
 	size_t size;
@@ -145,6 +179,12 @@ u64 omap_gem_mmap_offset(struct drm_gem_object *obj)
 		return 0;
 	}
 
+	if (omap_obj->file && omap_obj->handle) {
+		ret = drm_vma_node_allow(&obj->vma_node, omap_obj->file);
+		if (ret)
+			return ret;
+	}
+
 	return drm_vma_node_offset_addr(&obj->vma_node);
 }
 
@@ -338,6 +378,18 @@ size_t omap_gem_mmap_size(struct drm_gem_object *obj)
 	return size;
 }
 
+/* get tiled size, returns -EINVAL if not tiled buffer */
+int omap_gem_tiled_size(struct drm_gem_object *obj, uint16_t *w, uint16_t *h)
+{
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+	if (omap_obj->flags & OMAP_BO_TILED_MASK) {
+		*w = omap_obj->width;
+		*h = omap_obj->height;
+		return 0;
+	}
+	return -EINVAL;
+}
+
 /* -----------------------------------------------------------------------------
  * Fault Handling
  */
@@ -571,6 +623,9 @@ int omap_gem_mmap_obj(struct drm_gem_object *obj,
 		vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 	}
 
+	if (omap_obj->ops && omap_obj->ops->mmap)
+		omap_obj->ops->mmap(obj->filp, vma);
+
 	return 0;
 }
 
@@ -764,7 +819,7 @@ void omap_gem_dma_sync_buffer(struct drm_gem_object *obj,
  *
  * Return 0 on success or a negative error code otherwise.
  */
-int omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr)
+int omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr, bool remap)
 {
 	struct omap_drm_private *priv = obj->dev->dev_private;
 	struct omap_gem_object *omap_obj = to_omap_bo(obj);
@@ -772,7 +827,7 @@ int omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr)
 
 	mutex_lock(&omap_obj->lock);
 
-	if (!omap_gem_is_contiguous(omap_obj) && priv->has_dmm) {
+	if (!omap_gem_is_contiguous(omap_obj) && remap && priv->has_dmm) {
 		if (refcount_read(&omap_obj->dma_addr_cnt) == 0) {
 			u32 npages = obj->size >> PAGE_SHIFT;
 			enum tiler_fmt fmt = gem2fmt(omap_obj->flags);
@@ -789,7 +844,7 @@ int omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr)
 			if (omap_obj->flags & OMAP_BO_TILED_MASK) {
 				block = tiler_reserve_2d(fmt,
 						omap_obj->width,
-						omap_obj->height, 0);
+						omap_obj->height, PAGE_SIZE);
 			} else {
 				block = tiler_reserve_1d(obj->size);
 			}
@@ -1085,6 +1140,233 @@ void omap_gem_describe_objects(struct list_head *list, struct seq_file *m)
 }
 #endif
 
+/* -----------------------------------------------------------------------------
+ * Buffer Synchronization
+ */
+
+static DEFINE_SPINLOCK(sync_lock);
+
+struct omap_gem_sync_waiter {
+	struct list_head list;
+	struct omap_gem_object *omap_obj;
+	enum omap_gem_op op;
+	/* notify called w/ sync_lock held */
+	void (*notify)(void *arg);
+	void *arg;
+};
+
+/* list of omap_gem_sync_waiter.. the notify fxn gets called back when
+ * the read and/or write target count is achieved which can call a user
+ * callback (ex. to kick 3d and/or 2d), wakeup blocked task (prep for
+ * cpu access), etc.
+ */
+static LIST_HEAD(waiters);
+
+static inline bool is_waiting(struct omap_gem_sync_waiter *waiter)
+{
+	struct omap_gem_object *omap_obj = waiter->omap_obj;
+	if ((waiter->op & OMAP_GEM_READ) &&
+			(omap_obj->sync->write_complete < omap_obj->sync->write_pending))
+		return true;
+	if ((waiter->op & OMAP_GEM_WRITE) &&
+			(omap_obj->sync->read_complete < omap_obj->sync->read_pending))
+		return true;
+	return false;
+}
+
+/* macro for sync debug.. */
+#define SYNCDBG 0
+#define SYNC(fmt, ...) do { if (SYNCDBG)				\
+		pr_err("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__); \
+	} while (0)
+
+
+static void sync_op_update(void)
+{
+	struct omap_gem_sync_waiter *waiter, *n;
+	list_for_each_entry_safe(waiter, n, &waiters, list) {
+		if (!is_waiting(waiter)) {
+			list_del(&waiter->list);
+			SYNC("notify: %p", waiter);
+			waiter->notify(waiter->arg);
+			kfree(waiter);
+		}
+	}
+}
+
+static inline int sync_op(struct drm_gem_object *obj,
+		enum omap_gem_op op, bool start)
+{
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+	int ret = 0;
+
+	spin_lock(&sync_lock);
+
+	if (!omap_obj->sync) {
+		omap_obj->sync = kzalloc(sizeof(*omap_obj->sync), GFP_ATOMIC);
+		if (!omap_obj->sync) {
+			ret = -ENOMEM;
+			goto unlock;
+		}
+	}
+
+	if (start) {
+		if (op & OMAP_GEM_READ)
+			omap_obj->sync->read_pending++;
+		if (op & OMAP_GEM_WRITE)
+			omap_obj->sync->write_pending++;
+	} else {
+		if (op & OMAP_GEM_READ)
+			omap_obj->sync->read_complete++;
+		if (op & OMAP_GEM_WRITE)
+			omap_obj->sync->write_complete++;
+		sync_op_update();
+	}
+
+unlock:
+	spin_unlock(&sync_lock);
+
+	return ret;
+}
+
+/* it is a bit lame to handle updates in this sort of polling way, but
+ * in case of PVR, the GPU can directly update read/write complete
+ * values, and not really tell us which ones it updated.. this also
+ * means that sync_lock is not quite sufficient.  So we'll need to
+ * do something a bit better when it comes time to add support for
+ * separate 2d hw..
+ */
+void omap_gem_op_update(void)
+{
+	spin_lock(&sync_lock);
+	sync_op_update();
+	spin_unlock(&sync_lock);
+}
+
+/* mark the start of read and/or write operation */
+int omap_gem_op_start(struct drm_gem_object *obj, enum omap_gem_op op)
+{
+	return sync_op(obj, op, true);
+}
+
+/* end of read and/or write operation, update dsi command mode panel */
+int omap_gem_op_finish(struct drm_gem_object *obj, enum omap_gem_op op)
+{
+	struct omap_drm_private *priv = obj->dev->dev_private;
+	struct drm_crtc *crtc = priv->pipes[0].crtc;
+	int error;
+
+	error = sync_op(obj, op, false);
+	omap_crtc_flush(crtc);
+
+	return error;
+}
+
+static DECLARE_WAIT_QUEUE_HEAD(sync_event);
+
+static void sync_notify(void *arg)
+{
+	struct task_struct **waiter_task = arg;
+	*waiter_task = NULL;
+	wake_up_all(&sync_event);
+}
+
+int omap_gem_op_sync(struct drm_gem_object *obj, enum omap_gem_op op)
+{
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+	int ret = 0;
+	if (omap_obj->sync) {
+		struct task_struct *waiter_task = current;
+		struct omap_gem_sync_waiter *waiter =
+				kzalloc(sizeof(*waiter), GFP_KERNEL);
+
+		if (!waiter)
+			return -ENOMEM;
+
+		waiter->omap_obj = omap_obj;
+		waiter->op = op;
+		waiter->notify = sync_notify;
+		waiter->arg = &waiter_task;
+
+		spin_lock(&sync_lock);
+		if (is_waiting(waiter)) {
+			SYNC("waited: %p", waiter);
+			list_add_tail(&waiter->list, &waiters);
+			spin_unlock(&sync_lock);
+			ret = wait_event_interruptible(sync_event,
+					(waiter_task == NULL));
+			spin_lock(&sync_lock);
+			if (waiter_task) {
+				SYNC("interrupted: %p", waiter);
+				/* we were interrupted */
+				list_del(&waiter->list);
+				waiter_task = NULL;
+			} else {
+				/* freed in sync_op_update() */
+				waiter = NULL;
+			}
+		}
+		spin_unlock(&sync_lock);
+		kfree(waiter);
+	}
+	return ret;
+}
+
+/* call fxn(arg), either synchronously or asynchronously if the op
+ * is currently blocked..  fxn() can be called from any context
+ *
+ * (TODO for now fxn is called back from whichever context calls
+ * omap_gem_op_update().. but this could be better defined later
+ * if needed)
+ *
+ * TODO more code in common w/ _sync()..
+ */
+int omap_gem_op_async(struct drm_gem_object *obj, enum omap_gem_op op,
+		void (*fxn)(void *arg), void *arg)
+{
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+	if (omap_obj->sync) {
+		struct omap_gem_sync_waiter *waiter =
+				kzalloc(sizeof(*waiter), GFP_ATOMIC);
+
+		if (!waiter)
+			return -ENOMEM;
+
+		waiter->omap_obj = omap_obj;
+		waiter->op = op;
+		waiter->notify = fxn;
+		waiter->arg = arg;
+
+		spin_lock(&sync_lock);
+		if (is_waiting(waiter)) {
+			SYNC("waited: %p", waiter);
+			list_add_tail(&waiter->list, &waiters);
+			spin_unlock(&sync_lock);
+			return 0;
+		}
+
+		spin_unlock(&sync_lock);
+
+		kfree(waiter);
+	}
+
+	/* no waiting.. */
+	fxn(arg);
+
+	return 0;
+}
+
+/* Old unused special API so PVR can update the buffer to use a sync-object
+ * allocated from it's sync-obj heap.  Only used for a newly allocated (from
+ * PVR's perspective) sync-object, so we overwrite the new syncobj w/ values
+ * from the already allocated syncobj (if there is one). In pvr-omap4-dkms
+ * we have had PVRSRV_DISABLE_UM_SYNCOBJ_MAPPINGS, so this now unused.
+ */
+int omap_gem_set_sync_object(struct drm_gem_object *obj, void *syncobj)
+{
+	return 0;
+}
+
 /* -----------------------------------------------------------------------------
  * Constructor & Destructor
  */
@@ -1101,6 +1383,9 @@ void omap_gem_free_object(struct drm_gem_object *obj)
 	list_del(&omap_obj->mm_list);
 	mutex_unlock(&priv->list_lock);
 
+	if (omap_obj->flags & OMAP_BO_MEM_PIN)
+		omap_gem_unpin_locked(obj);
+
 	/*
 	 * We own the sole reference to the object at this point, but to keep
 	 * lockdep happy, we must still take the omap_obj_lock to call
@@ -1112,6 +1397,10 @@ void omap_gem_free_object(struct drm_gem_object *obj)
 	/* The object should not be pinned. */
 	WARN_ON(refcount_read(&omap_obj->dma_addr_cnt) > 0);
 
+	/* don't free externally allocated backing memory */
+	if (omap_obj->flags & OMAP_BO_MEM_EXT)
+		goto check_bo_ext_sync;
+
 	if (omap_obj->pages) {
 		if (omap_obj->flags & OMAP_BO_MEM_DMABUF)
 			kfree(omap_obj->pages);
@@ -1130,6 +1419,11 @@ void omap_gem_free_object(struct drm_gem_object *obj)
 
 	mutex_unlock(&omap_obj->lock);
 
+check_bo_ext_sync:
+	/* don't free externally allocated syncobj */
+	if (!(omap_obj->flags & OMAP_BO_EXT_SYNC))
+		kfree(omap_obj->sync);
+
 	drm_gem_object_release(obj);
 
 	mutex_destroy(&omap_obj->lock);
@@ -1151,10 +1445,19 @@ static bool omap_gem_validate_flags(struct drm_device *dev, u32 flags)
 		return false;
 	}
 
+	if ((flags & OMAP_BO_MEM_CONTIG) && (flags & OMAP_BO_MEM_DMM))
+		return false;
+
+	if ((flags & OMAP_BO_MEM_DMM) && !priv->usergart)
+		return false;
+
 	if (flags & OMAP_BO_TILED_MASK) {
 		if (!priv->usergart)
 			return false;
 
+		if (flags & OMAP_BO_MEM_CONTIG)
+			return false;
+
 		switch (flags & OMAP_BO_TILED_MASK) {
 		case OMAP_BO_TILED_8:
 		case OMAP_BO_TILED_16:
@@ -1169,7 +1472,34 @@ static bool omap_gem_validate_flags(struct drm_device *dev, u32 flags)
 	return true;
 }
 
-/* GEM buffer object constructor */
+/**
+ * omap_gem_new() - Create a new GEM buffer
+ * @dev: The DRM device
+ * @gsize: The requested size for the GEM buffer. If the buffer is tiled
+ *         (2D buffer), the size is a pair of values: height and width
+ *         expressed in pixels. If the buffers is not tiled, it is expressed
+ *         in bytes.
+ * @flags: Flags give additionnal information about the allocation:
+ *         OMAP_BO_TILED_x: use the TILER (2D buffers). The TILER container
+ *              unit can be 8, 16 or 32 bits. Cache is always disabled for
+ *              tiled buffers.
+ *         OMAP_BO_SCANOUT: Scannout buffer, consummable by the DSS
+ *         OMAP_BO_CACHED: Buffer CPU caching mode: cached
+ *         OMAP_BO_WC: Buffer CPU caching mode: write-combined
+ *         OMAP_BO_UNCACHED: Buffer CPU caching mode: uncached
+ *         OMAP_BO_MEM_CONTIG: The driver will use dma_alloc to get the memory.
+ *              This can be used to avoid DMM if the userspace knows it needs
+ *              more than 128M of memory at the same time.
+ *         OMAP_BO_MEM_DMM: The driver will use DMM to get the memory. There's
+ *              not much use for this flag at the moment, as on platforms with
+ *              DMM it is used by default, but it's here for completeness.
+ *         OMAP_BO_MEM_PIN: The driver will pin the memory at alloc time, and
+ *              keep it pinned. This can be used to 1) get an error at alloc
+ *              time if DMM space is full, and 2) get rid of the constant
+ *              pin/unpin operations which may have some effect on performance.
+ *
+ * Return: The GEM buffer or NULL if the allocation failed
+ */
 struct drm_gem_object *omap_gem_new(struct drm_device *dev,
 		union omap_gem_size gsize, u32 flags)
 {
@@ -1197,15 +1527,17 @@ struct drm_gem_object *omap_gem_new(struct drm_device *dev,
 		 */
 		flags &= ~(OMAP_BO_CACHED|OMAP_BO_WC|OMAP_BO_UNCACHED);
 		flags |= tiler_get_cpu_cache_flags();
-	} else if ((flags & OMAP_BO_SCANOUT) && !priv->has_dmm) {
+	} else if ((flags & OMAP_BO_MEM_CONTIG) ||
+		((flags & OMAP_BO_SCANOUT) && !priv->has_dmm)) {
 		/*
 		 * If we don't have DMM, we must allocate scanout buffers
 		 * from contiguous DMA memory.
 		 */
 		flags |= OMAP_BO_MEM_DMA_API;
-	} else if (!(flags & OMAP_BO_MEM_DMABUF)) {
+	} else if (!(flags & (OMAP_BO_MEM_EXT | OMAP_BO_MEM_DMABUF))) {
 		/*
-		 * All other buffers not backed by dma_buf are shmem-backed.
+		 * All other buffers not backed by external memory or dma_buf
+		 * are shmem-backed.
 		 */
 		flags |= OMAP_BO_MEM_SHMEM;
 	}
@@ -1257,12 +1589,22 @@ struct drm_gem_object *omap_gem_new(struct drm_device *dev,
 			goto err_release;
 	}
 
+	if (flags & OMAP_BO_MEM_PIN) {
+		ret = omap_gem_pin(obj, NULL, false);
+		if (ret)
+			goto err_free_dma;
+	}
+
 	mutex_lock(&priv->list_lock);
 	list_add(&omap_obj->mm_list, &priv->obj_list);
 	mutex_unlock(&priv->list_lock);
 
 	return obj;
 
+err_free_dma:
+	if (flags & OMAP_BO_MEM_DMA_API)
+		dma_free_wc(dev->dev, size, omap_obj->vaddr,
+			    omap_obj->dma_addr);
 err_release:
 	drm_gem_object_release(obj);
 err_free:
@@ -1424,3 +1766,84 @@ void omap_gem_deinit(struct drm_device *dev)
 	 */
 	kfree(priv->usergart);
 }
+
+EXPORT_SYMBOL(omap_gem_flags);
+EXPORT_SYMBOL(omap_gem_mmap_offset);
+EXPORT_SYMBOL(omap_gem_tiled_size);
+EXPORT_SYMBOL(omap_gem_pin);
+EXPORT_SYMBOL(omap_gem_unpin);
+EXPORT_SYMBOL(omap_gem_tiled_stride);
+EXPORT_SYMBOL(omap_gem_get_pages);
+EXPORT_SYMBOL(omap_gem_put_pages);
+EXPORT_SYMBOL(omap_gem_op_update);
+EXPORT_SYMBOL(omap_gem_op_async);
+EXPORT_SYMBOL(omap_gem_set_sync_object);
+
+/*
+ * This constructor is mainly to give plugins a way to wrap their
+ * own allocations
+ */
+struct drm_gem_object *omap_gem_new_ext(struct drm_device *dev,
+		struct drm_file *file, u32 *handle,
+		union omap_gem_size gsize, uint32_t flags,
+		dma_addr_t dma_addr, struct page **pages,
+		struct omap_gem_vm_ops *ops)
+{
+	struct omap_gem_object *omap_obj;
+	struct drm_gem_object *obj;
+
+	BUG_ON((flags & OMAP_BO_TILED_MASK) && !pages);
+	if (dma_addr) {
+		dev_info(dev->dev, "%s dma_addr: %08x\n", __func__, dma_addr);
+		flags |= OMAP_BO_MEM_DMABUF;
+	}
+	obj = omap_gem_new(dev, gsize, flags | OMAP_BO_MEM_EXT);
+
+	if (obj) {
+		omap_obj = to_omap_bo(obj);
+
+		omap_obj->dma_addr = dma_addr;
+		omap_obj->pages = pages;
+		omap_obj->ops = ops;
+		omap_obj->file = file;
+		omap_obj->handle = handle;
+	}
+	return obj;
+}
+EXPORT_SYMBOL(omap_gem_new_ext);
+
+void omap_gem_vm_open(struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+
+	if (omap_obj->ops && omap_obj->ops->open)
+		omap_obj->ops->open(vma);
+	else
+		drm_gem_vm_open(vma);
+
+}
+
+void omap_gem_vm_close(struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+
+	if (omap_obj->ops && omap_obj->ops->close)
+		omap_obj->ops->close(vma);
+	else
+		drm_gem_vm_close(vma);
+
+}
+
+void *omap_gem_priv(struct drm_gem_object *obj)
+{
+	return to_omap_bo(obj)->priv;
+}
+EXPORT_SYMBOL(omap_gem_priv);
+
+void omap_gem_set_priv(struct drm_gem_object *obj, void *priv)
+{
+	to_omap_bo(obj)->priv = priv;
+}
+EXPORT_SYMBOL(omap_gem_set_priv);
diff --git a/drivers/gpu/drm/omapdrm/omap_gem.h b/drivers/gpu/drm/omapdrm/omap_gem.h
--- a/drivers/gpu/drm/omapdrm/omap_gem.h
+++ b/drivers/gpu/drm/omapdrm/omap_gem.h
@@ -61,7 +61,6 @@ int omap_gem_dumb_create(struct drm_file *file, struct drm_device *dev,
 int omap_gem_mmap(struct file *filp, struct vm_area_struct *vma);
 int omap_gem_mmap_obj(struct drm_gem_object *obj,
 		struct vm_area_struct *vma);
-u64 omap_gem_mmap_offset(struct drm_gem_object *obj);
 size_t omap_gem_mmap_size(struct drm_gem_object *obj);
 
 /* PRIME Interface */
@@ -74,15 +73,8 @@ int omap_gem_roll(struct drm_gem_object *obj, u32 roll);
 void omap_gem_cpu_sync_page(struct drm_gem_object *obj, int pgoff);
 void omap_gem_dma_sync_buffer(struct drm_gem_object *obj,
 		enum dma_data_direction dir);
-int omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr);
-void omap_gem_unpin(struct drm_gem_object *obj);
-int omap_gem_get_pages(struct drm_gem_object *obj, struct page ***pages,
-		bool remap);
-int omap_gem_put_pages(struct drm_gem_object *obj);
 
-u32 omap_gem_flags(struct drm_gem_object *obj);
 int omap_gem_rotated_dma_addr(struct drm_gem_object *obj, u32 orient,
 		int x, int y, dma_addr_t *dma_addr);
-int omap_gem_tiled_stride(struct drm_gem_object *obj, u32 orient);
 
 #endif /* __OMAPDRM_GEM_H__ */
diff --git a/drivers/gpu/drm/omapdrm/omap_gem_dmabuf.c b/drivers/gpu/drm/omapdrm/omap_gem_dmabuf.c
--- a/drivers/gpu/drm/omapdrm/omap_gem_dmabuf.c
+++ b/drivers/gpu/drm/omapdrm/omap_gem_dmabuf.c
@@ -31,7 +31,7 @@ static struct sg_table *omap_gem_map_dma_buf(
 	/* camera, etc, need physically contiguous.. but we need a
 	 * better way to know this..
 	 */
-	ret = omap_gem_pin(obj, &dma_addr);
+	ret = omap_gem_pin(obj, &dma_addr, true);
 	if (ret)
 		goto out;
 
diff --git a/include/uapi/drm/omap_drm.h b/include/uapi/drm/omap_drm.h
--- a/include/uapi/drm/omap_drm.h
+++ b/include/uapi/drm/omap_drm.h
@@ -47,6 +47,15 @@ struct drm_omap_param {
 #define OMAP_BO_UNCACHED	0x00000004
 #define OMAP_BO_CACHE_MASK	0x00000006
 
+/* Force allocation from contiguous DMA memory */
+#define OMAP_BO_MEM_CONTIG	0x00000008
+
+/* Force allocation via DMM */
+#define OMAP_BO_MEM_DMM		0x00000010
+
+/* Pin the buffer when allocating and keep pinned */
+#define OMAP_BO_MEM_PIN		0x00000020
+
 /* Use TILER for the buffer. The TILER container unit can be 8, 16 or 32 bits. */
 #define OMAP_BO_TILED_8		0x00000100
 #define OMAP_BO_TILED_16	0x00000200
@@ -107,8 +116,8 @@ struct drm_omap_gem_info {
 #define DRM_OMAP_GET_PARAM		0x00
 #define DRM_OMAP_SET_PARAM		0x01
 #define DRM_OMAP_GEM_NEW		0x03
-#define DRM_OMAP_GEM_CPU_PREP		0x04	/* Deprecated, to be removed */
-#define DRM_OMAP_GEM_CPU_FINI		0x05	/* Deprecated, to be removed */
+#define DRM_OMAP_GEM_CPU_PREP		0x04
+#define DRM_OMAP_GEM_CPU_FINI		0x05
 #define DRM_OMAP_GEM_INFO		0x06
 #define DRM_OMAP_NUM_IOCTLS		0x07
 
@@ -119,6 +128,81 @@ struct drm_omap_gem_info {
 #define DRM_IOCTL_OMAP_GEM_CPU_FINI	DRM_IOW (DRM_COMMAND_BASE + DRM_OMAP_GEM_CPU_FINI, struct drm_omap_gem_cpu_fini)
 #define DRM_IOCTL_OMAP_GEM_INFO		DRM_IOWR(DRM_COMMAND_BASE + DRM_OMAP_GEM_INFO, struct drm_omap_gem_info)
 
+/* interface that plug-in drivers (for now just PVR) can implement */
+struct omap_drm_plugin {
+	struct drm_device *dev;
+	const char *name;
+
+	/* drm functions */
+	int (*load)(struct drm_device *dev, unsigned long flags);
+	int (*unload)(struct drm_device *dev);
+	int (*open)(struct drm_device *dev, struct drm_file *file);
+	void (*release)(struct drm_device *dev, struct drm_file *file);
+
+	const struct drm_ioctl_desc *ioctls;
+	int num_ioctls;
+	int ioctl_base;
+};
+
+int omap_drm_register_plugin(struct omap_drm_plugin *plugin);
+int omap_drm_unregister_plugin(struct omap_drm_plugin *plugin);
+
+void *omap_drm_file_priv(struct drm_file *file);
+void omap_drm_file_set_priv(struct drm_file *file, void *priv);
+
+void *omap_gem_priv(struct drm_gem_object *obj);
+void omap_gem_set_priv(struct drm_gem_object *obj, void *priv);
+void omap_gem_vm_open(struct vm_area_struct *vma);
+void omap_gem_vm_close(struct vm_area_struct *vma);
+u64 omap_gem_mmap_offset(struct drm_gem_object *obj);
+u32 omap_gem_flags(struct drm_gem_object *obj);
+int omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr, bool remap);
+void omap_gem_unpin(struct drm_gem_object *obj);
+int omap_gem_get_pages(struct drm_gem_object *obj, struct page ***pages,
+		bool remap);
+int omap_gem_put_pages(struct drm_gem_object *obj);
+int omap_gem_op_start(struct drm_gem_object *obj, enum omap_gem_op op);
+int omap_gem_op_finish(struct drm_gem_object *obj, enum omap_gem_op op);
+int omap_gem_op_sync(struct drm_gem_object *obj, enum omap_gem_op op);
+int omap_gem_op_async(struct drm_gem_object *obj, enum omap_gem_op op,
+		      void (*fxn)(void *arg), void *arg);
+int omap_gem_tiled_size(struct drm_gem_object *obj, uint16_t *w, uint16_t *h);
+int omap_gem_tiled_stride(struct drm_gem_object *obj, u32 orient);
+
+/* for external plugin buffers wrapped as GEM object (via. omap_gem_new_ext())
+ * a vm_ops struct can be provided to get callback notification of various
+ * events..
+ */
+struct omap_gem_vm_ops {
+	void (*open)(struct vm_area_struct *area);
+	void (*close)(struct vm_area_struct *area);
+	/*maybe: int (*fault)(struct vm_area_struct *vma,
+	  struct vm_fault *vmf)*/
+
+	/* note: mmap isn't expected to do anything. it is just to allow buffer
+	 * allocate to update it's own internal state
+	 */
+	void (*mmap)(struct file *, struct vm_area_struct *);
+};
+
+struct drm_gem_object *omap_gem_new_ext(struct drm_device *dev,
+		struct drm_file *file, u32 *handle,
+		union omap_gem_size gsize, uint32_t flags,
+		dma_addr_t paddr, struct page **pages,
+		struct omap_gem_vm_ops *ops);
+
+void omap_gem_op_update(void);
+int omap_gem_set_sync_object(struct drm_gem_object *obj, void *syncobj);
+
+struct drm_omap_get_base {
+	char plugin_name[64];	/* in */
+	uint32_t ioctl_base;	/* out */
+	uint32_t __pad;
+};
+
+#define DRM_IOCTL_OMAP_GET_BASE		DRM_IOWR(DRM_COMMAND_BASE + DRM_OMAP_GET_BASE, struct drm_omap_get_base)
+#define DRM_OMAP_GET_BASE		0x02
+
 #if defined(__cplusplus)
 }
 #endif
-- 
2.25.0
